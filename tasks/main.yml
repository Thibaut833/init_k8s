---
# tasks file for init_k8s

- name: J'installe le paquet open-iscsi et python3-kubernetes
  ansible.builtin.apt:
    name:
      - open-iscsi
      - python3-kubernetes
    state: present
    update_cache: true

- name: Je configure le cluster sur le controlplane
  delegate_to: "{{ play_hosts | first }}"
  run_once: true
  block:
    - name: Je vérifie si le cluster tourne
      ansible.builtin.uri:
        url: "https://{{ ansible_host }}:6443/healthz"
        method: GET
        validate_certs: false
        status_code: ["200", "-1"]
      register: check_k8s
      ignore_errors: true

    - name: J'initialise le cluster kube sur le controlplane
      ansible.builtin.command:
        cmd: kubeadm init --pod-network-cidr=10.244.0.0/16
      register: init_controlplane
      changed_when: init_controlplane.rc == 0
      when:
        - check_k8s.status != 200
        - check_k8s.json is not defined or check_k8s.json != "ok"

    - name: Je mets l'export kubeconfig dans zshrc du controlplane
      ansible.builtin.lineinfile:
        path: ~/.zshrc
        insertafter: EOF
        line: "export KUBECONFIG=/etc/kubernetes/admin.conf"

    - name: Je crée la commande pour rejoindre le controlplane
      ansible.builtin.set_fact:
        kubeadm_join: "{{ init_controlplane.stdout_lines[-2:] | join('') | regex_replace('\\\\\\t', '') }}"
      when:
        - check_k8s.status != 200
        - check_k8s.json is not defined or check_k8s.json != "ok"

    - name: Je crée la commande pour rejoindre le controlplane dans un fichier
      ansible.builtin.copy:
        content: "{{ kubeadm_join }}"
        dest: /srv/kubeadm_join.txt
        mode: "0660"
        owner: "root"
        group: "root"
      when:
        - check_k8s.status != 200
        - check_k8s.json is not defined or check_k8s.json != "ok"

    - name: Je recupère le contenu du fichier kubeadm_join.txt
      ansible.builtin.slurp:
        src: /srv/kubeadm_join.txt
      register: kubeadm_join_slurp

    - name: Je regarde si le fichier encryptionConfig.yaml existe
      ansible.builtin.stat:
        path: "/etc/kubernetes/enc/encryptionConfig.yaml"
      register: exists_encryptionconfig

    - name: Je chiffre la base etcd
      when: not exists_encryptionconfig.stat.exists
      block:
        - name: Je crée la clé de chiffrement pour etcd
          ansible.builtin.shell:
            cmd: "head -c 32 /dev/urandom | base64"
          register: key_chiff          

        - name: Je crée le dossier enc dans /etc/kubernetes/
          ansible.builtin.file:
            path: /etc/kubernetes/enc
            state: directory
            recurse: true
            owner: root
            group: root

        - name: Je copie le template encryptionConfig dans le dossier
          ansible.builtin.template:
            src: "{{ role_path }}/templates/encryptionConfig.j2"
            dest: /etc/kubernetes/enc/encryptionConfig.yaml
            mode: "770"

        - name: Je lis la configuration de kube-apiserver
          ansible.builtin.slurp:
            src: /etc/kubernetes/manifests/kube-apiserver.yaml
          register: conf_kubeapiserv  

        - name: Je mets la conf dans une var
          ansible.builtin.set_fact:
            conf_kubeapiserv_work: "{{ conf_kubeapiserv['content'] | b64decode | from_yaml }}"

        - name: J'ajoute l'arg encryption sur la commande kube-apiserver
          ansible.builtin.set_fact:
            newcommandekubeapi: "{{ conf_kubeapiserv_work.spec.containers[0].command + ['--encryption-provider-config=/etc/kubernetes/enc/encryptionConfig.yaml']}}"

        - name: Je remplace la liste de commande pour kube-apiserver avec la nouvelle
          ansible.builtin.set_fact:
            conf_kubeapiserv_work: "{{ conf_kubeapiserv_work | replace(conf_kubeapiserv_work.spec.containers[0].command, newcommandekubeapi) }}"

        - name: J'ajoute le volumeMount enc
          ansible.builtin.set_fact:
            newvmlist: "{{ conf_kubeapiserv_work.spec.containers[0].volumeMounts + vm_enc }}"
          vars:
            vm_enc:
              - name: enc
                mountPath: /etc/kubernetes/enc
                readOnly: true

        - name: Je remplace la liste de volumeMount par la nouvelle
          ansible.builtin.set_fact:
            conf_kubeapiserv_work: "{{ conf_kubeapiserv_work | replace(conf_kubeapiserv_work.spec.containers[0].volumeMounts, newvmlist) }}"

        - name: J'ajoute le volume enc
          set_fact:
            newvlist: "{{ conf_kubeapiserv_work.spec.volumes + v_enc }}"
          vars:
            v_enc:
              - name: enc
                hostPath:
                  path: /etc/kubernetes/enc
                  type: DirectoryOrCreate

        - name: Je remplace la liste de volumes par la nouvelle
          set_fact:
            conf_kubeapiserv_work: "{{ conf_kubeapiserv_work | replace(conf_kubeapiserv_work.spec.volumes, newvlist) }}"
      
        - name: Je mets à jour mon manifests pod static kube-apiserver
          ansible.builtin.copy:
            content: "{{ conf_kubeapiserv_work | to_yaml  }}"
            dest: /etc/kubernetes/manifests/kube-apiserver.yaml

        - name: Je vérifie la connexion à l'api
          ansible.builtin.wait_for: 
            port: 6443
            delay: 30

        - name: Je vérifie que kubectl fonctionne
          ansible.builtin.shell: kubectl get nodes --kubeconfig=/etc/kubernetes/admin.conf
          register: check_kubectl
          until:      
            - check_kubectl.rc  == 0      
          retries: 30
          delay: 10

    - name: J'installe Calico
      block:
        - name: J'installe l'operateur Calico
          kubernetes.core.k8s:
            state: present
            kubeconfig: /etc/kubernetes/admin.conf
            src: https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml

    - name: Je lance la commande pour récupérer le nombre de nodes
      ansible.builtin.shell: kubectl get --kubeconfig=/etc/kubernetes/admin.conf nodes --no-headers | wc -l
      register: num_nodes_raw

    - name: J'extrait le nombre de nodes
      ansible.builtin.set_fact:
        num_nodes: "{{ num_nodes_raw.stdout | int }}"

- name: Je fais rejoindre les noeuds sur le cluster kube
  ansible.builtin.command:
    cmd: "{% if hostvars[groups['kub'][0]]['kubeadm_join'] is not defined %}{{ hostvars[groups['kub'][0]]['kubeadm_join_slurp']['content'] | b64decode }}{% else %}{{ hostvars[groups['kub'][0]]['kubeadm_join'] }}{% endif %}"
  register: workers_join_cluster
  changed_when: workers_join_cluster.rc == 0
  delegate_to: "{{ item }}"
  run_once: true
  loop: "{{ play_hosts[1:] }}"
  no_log: true
  ignore_errors: true

- name: J'installe Helm
  delegate_to: "{{ play_hosts | first }}"
  run_once: true
  block:
    - name: J'ajoute la clé du repo helm
      ansible.builtin.get_url:
        url: https://baltocdn.com/helm/signing.asc
        dest: /tmp/helm-signing.asc
        mode: "a+r"
        force: "true"

    - name: Je vérifie la présence de la clé gpg helm dans le folder apt keyrings
      ansible.builtin.stat:
        path: "/etc/apt/keyrings/helm.gpg"
      register: exists_helmasc

    - name: Je de-armor la clé helm
      ansible.builtin.shell: gpg --dearmor < /tmp/helm-signing.asc > /etc/apt/keyrings/helm.gpg
      when: not exists_helmasc.stat.exists

    - name: J'autorise la lecture pour tous sur la clé helm
      ansible.builtin.file:
        path: /etc/apt/keyrings/helm.gpg
        mode: "a+r"

    - name: J'ajoute le repository helm
      ansible.builtin.apt_repository:
        repo: "deb [arch=amd64 signed-by=/etc/apt/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main"

    - name: J'installe le paquet helm
      ansible.builtin.apt:
        name: helm
        state: present
        update_cache: true

- name: J'installe OpenEBS
  delegate_to: "{{ play_hosts | first }}"
  run_once: true
  block:
    - name: J'install open-iscsi
      ansible.builtin.apt:
        name: open-iscsi
        state: present
        update_cache: yes
    
    - name: J'active le service iscsid
      ansible.builtin.systemd_service:
        name: iscsid
        enabled: true
        state: started

    - name: Je lance le déploiement de openebs cstor operator
      kubernetes.core.k8s:
        state: present
        kubeconfig: /etc/kubernetes/admin.conf
        src: https://openebs.github.io/charts/cstor-operator.yaml
        wait: true
        wait_timeout: 480
        wait_sleep: 20

    - name: J'installe le paquet jq
      ansible.builtin.apt:
        name: jq
        update_cache: true
        state: present

    - name: Je récupère les blocs devices
      ansible.builtin.shell:
        cmd: "kubectl get bd -n openebs --kubeconfig=/etc/kubernetes/admin.conf -o=json | jq -c '.items[] | select(.spec.capacity.storage == {{ oebssizestordisk }}) | {bd: .metadata.name, node: .metadata.labels.nodename}'"
      register: bloc_devices
      when: oebssizestordisk is defined
      failed_when: oebssizestordisk is not defined

    - name: Je mets les bds dans une var nommée bdebs
      ansible.builtin.set_fact:
        bdebs: "{{ bloc_devices.stdout_lines | map('regex_replace', '\\\\', '') | list }}"

    - name: Je copie mon template pour le cstorpoolcluster
      ansible.builtin.template:
        src: "{{ role_path }}/templates/cspc.j2"
        dest: /srv/cspc.yaml
        mode: "660"

    - name: Je lance le déploiement de openebs cstorpoolcluster
      kubernetes.core.k8s:
        state: present
        kubeconfig: /etc/kubernetes/admin.conf
        src: /srv/cspc.yaml

    - name: Je copie mon template pour le cstor-csi-disk
      ansible.builtin.template:
        src: "{{ role_path }}/templates/cstor-csi-disk.j2"
        dest: /srv/cstor-csi-disk.yaml
        mode: "660"

    - name: Je lance le déploiement de openebs cstorpoolcluster
      kubernetes.core.k8s:
        state: present
        kubeconfig: /etc/kubernetes/admin.conf
        src: /srv/cstor-csi-disk.yaml

- name: J'installe Velero
  delegate_to: "{{ play_hosts | first }}"
  run_once: true
  block:
    - name: Je télécharge l'archive Velero
      ansible.builtin.get_url:
        url: https://github.com/vmware-tanzu/velero/releases/download/v1.13.2/velero-v1.13.2-linux-amd64.tar.gz
        dest: /tmp/velero.tar.gz
        mode: '0770'

    - name: Je décompresse l'archive Velero
      ansible.builtin.unarchive:
        src: /tmp/velero.tar.gz
        dest: /usr/local/bin
        extra_opts: ['--strip-components=1', '--show-stored-names']
        include: 
          - velero-v1.13.2-linux-amd64/velero
        mode: "0710"
        remote_src: true

    - name: Je crée le fichier credentials-velero
      ansible.builtin.template:
        src: "{{ role_path }}/templates/credentials-velero.j2"
        dest: /srv/credentials-velero
        mode: "770"

    - name: Je vérifie les namespaces
      ansible.builtin.shell:
        cmd: kubectl get namespaces --kubeconfig /etc/kubernetes/admin.conf -o json
      register: velero_allns


    - name: Je verifie si velero est présent dans les namespaces
      ansible.builtin.set_fact:
        velero_checkns: "{{'velero' in velero_allns.stdout }}"

    - name: Je lance l'installation de Velero
      ansible.builtin.shell:
        cmd: >
          velero install
          --kubeconfig /etc/kubernetes/admin.conf --wait
          --use-node-agent --provider aws --plugins velero/velero-plugin-for-aws:v1.9.2
          --features=EnableCSI --bucket backupvol --secret-file /srv/credentials-velero 
          --backup-location-config region="{{ aws_s3_region }}",s3ForcePathStyle="true",s3Url="{{ aws_s3_endpoints }}"
      when: velero_checkns is false

    - name: Je vérifie les schedules velero
      ansible.builtin.shell:
        cmd: velero schedule get --kubeconfig /etc/kubernetes/admin.conf -o json
      register: velero_schedules

    - debug:
        msg: "{{ velero_schedules.stdout }}"

    - debug:
        msg: "{{ velero_schedules.stdout.items[*] | length }}"

    - name: J'installe les schedules velero
      ansible.builtin.shell:
        cmd: "{{ item }}"
      ignore_errors: true
      loop:
        - velero schedule create uptimekumabackup --kubeconfig /etc/kubernetes/admin.conf --schedule="0 21 * * *" --include-namespaces="uptime-kuma" --selector="app=uptime-kuma"
        - velero schedule create recipesbackup --kubeconfig /etc/kubernetes/admin.conf --schedule="0 21 * * *" --include-namespaces="recipes" --selector="app=recipes"
        - velero schedule create authentikbackup --kubeconfig /etc/kubernetes/admin.conf --schedule="0 21 * * *" --include-namespaces="authentik" --selector="app=postgres-authentik"

    - name: J'ajoute la configuration zshrc
      ansible.builtin.blockinfile:
        path: ~/.zshrc
        insertafter: EOF
        block: |
          source <(velero completion zsh)
          alias v=velero
          complete -F __start_velero v

- name: J'installe secrets sealed controller
  delegate_to: "{{ play_hosts | first }}"
  run_once: true  
  block:
    - name: J'installe le controleur
      kubernetes.core.k8s:
        state: present
        kubeconfig: /etc/kubernetes/admin.conf
        src: https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.26.1/controller.yaml

    - name: J'installe l'outil kubeseal
      ansible.builtin.get_url:
        url: https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.26.1/kubeseal-0.26.1-linux-amd64.tar.gz
        dest: /tmp/kubeseal.tar.gz
        mode: '0770'

    - name: Je décompresse l'archive
      ansible.builtin.unarchive:
        src: /tmp/kubeseal.tar.gz
        include: 
          - kubeseal
        dest: /usr/local/bin
        mode: "0710"
        remote_src: true

    - name: J'installe awscli
      ansible.builtin.apt:
        name: awscli
        state: present

    - name: Je check si la paire de clé kubeseal existe
      ansible.builtin.stat:
        path: /srv/kubesealkey
      register: check_kubesealkey

    - name: Je download depuis S3 ma paire de clé si elle n'est par sur le serveur
      ansible.builtin.shell:
        cmd: "{{ item }}"
      loop:
        - AWS_SECRET_ACCESS_KEY={{ aws_secret_access_key }} AWS_ACCESS_KEY_ID={{ aws_access_key_id }} aws --region={{ aws_s3_region }} --endpoint-url={{ aws_s3_endpoints }} s3 cp s3://{{ aws_s3_bucket }}/kubesealkey /srv/kubesealkey
        - AWS_SECRET_ACCESS_KEY={{ aws_secret_access_key }} AWS_ACCESS_KEY_ID={{ aws_access_key_id }} aws --region={{ aws_s3_region }} --endpoint-url={{ aws_s3_endpoints }} s3 cp s3://{{ aws_s3_bucket }}/kubesealkey.pub /srv/kubesealkey.pub
      when: not check_kubesealkey.stat.exists

    - name: Je recupère le contenu du fichier kubesealkey
      ansible.builtin.slurp:
        src: /srv/kubesealkey
      register: kubesealkey
    
    - name: Je recupère le contenu du fichier kubesealkey.pub
      ansible.builtin.slurp:
        src: /srv/kubesealkey.pub
      register: kubesealkeypub

    - name: Je crée le secret contenant les clés kubeseal
      kubernetes.core.k8s:
        state: present
        kubeconfig: /etc/kubernetes/admin.conf
        definition:
          apiVersion: v1
          kind: Secret
          type: kubernetes.io/tls
          metadata:
            name: kubesealkeys
            namespace: kube-system
            labels:
              sealedsecrets.bitnami.com/sealed-secrets-key: active
          data:
            tls.crt: "{{ kubesealkeypub.content }}"
            tls.key: "{{ kubesealkey.content }}"

    - name: Je supprime le controleur sealed secret pour qu'il se rebuild avec les clés kubeseal
      kubernetes.core.k8s:
        state: absent
        kubeconfig: /etc/kubernetes/admin.conf
        api_version: v1
        kind: Pod
        namespace: kube-system
        label_selectors:
          - "name=sealed-secrets-controller"

- name: J'installe fluxcd
  delegate_to: "{{ play_hosts | first }}"
  run_once: true
  block:
    - name: Je télécharge l'achive fluxcd
      ansible.builtin.get_url:
        url: https://github.com/fluxcd/flux2/releases/download/v2.2.3/flux_2.2.3_linux_amd64.tar.gz
        dest: /tmp/flux.tar.gz
        mode: '0770'

    - name: Je décompresse l'archive
      ansible.builtin.unarchive:
        src: /tmp/flux.tar.gz
        dest: /usr/local/bin
        mode: "0710"
        remote_src: true

    - name: Je check fluxcd
      ansible.builtin.shell:
        cmd: "flux check --pre --kubeconfig=/etc/kubernetes/admin.conf"
      register: fluxprecheck

    - name: J'affiche le résultat de fluxcd pre check
      ansible.builtin.debug:
        var: fluxprecheck

    - name: Flux bootstrap github
      when: 
        - fluxprecheck.rc == 0
        - githubtoken is defined
      ansible.builtin.shell:
        cmd: |
         echo "{{ githubtoken }}" | flux bootstrap github --token-auth --owner=Thibaut833 --repository=infra --branch=main --path=k8s --kubeconfig=/etc/kubernetes/admin.conf --personal